"""
è¯„ä¼°è„šæœ¬ï¼šè®¡ç®—æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½æŒ‡æ ‡
"""
import argparse
from pathlib import Path
import numpy as np
from PIL import Image
from tqdm import tqdm
import json


def calculate_metrics(pred_mask, gt_mask):
    """
    è®¡ç®—å•å¼ å›¾ç‰‡çš„æŒ‡æ ‡
    
    Args:
        pred_mask: é¢„æµ‹mask (0æˆ–1)
        gt_mask: çœŸå®mask (0æˆ–1)
    
    Returns:
        dict: åŒ…å«å„é¡¹æŒ‡æ ‡çš„å­—å…¸
    """
    pred_mask = pred_mask.astype(bool)
    gt_mask = gt_mask.astype(bool)
    
    # è®¡ç®—äº¤å¹¶æ¯”
    intersection = np.logical_and(pred_mask, gt_mask).sum()
    union = np.logical_or(pred_mask, gt_mask).sum()
    
    # IoU
    iou = intersection / union if union > 0 else 0.0
    
    # Diceç³»æ•°
    dice = 2 * intersection / (pred_mask.sum() + gt_mask.sum()) if (pred_mask.sum() + gt_mask.sum()) > 0 else 0.0
    
    # å‡†ç¡®ç‡
    accuracy = (pred_mask == gt_mask).sum() / gt_mask.size
    
    # ç²¾ç¡®ç‡å’Œå¬å›ç‡
    tp = intersection
    fp = (pred_mask & ~gt_mask).sum()
    fn = (gt_mask & ~pred_mask).sum()
    tn = (~pred_mask & ~gt_mask).sum()
    
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
    
    # F1 Score
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
    
    return {
        'iou': iou,
        'dice': dice,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1
    }


def evaluate(pred_dir, gt_dir, output_file=None):
    """
    è¯„ä¼°é¢„æµ‹ç»“æœ
    
    Args:
        pred_dir: é¢„æµ‹maskç›®å½•
        gt_dir: çœŸå®maskç›®å½•
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰
    """
    pred_dir = Path(pred_dir)
    gt_dir = Path(gt_dir)
    
    # è·å–æ‰€æœ‰é¢„æµ‹æ–‡ä»¶
    pred_files = sorted(list(pred_dir.glob('*.png')))
    
    if len(pred_files) == 0:
        print(f"âŒ åœ¨ {pred_dir} ä¸­æœªæ‰¾åˆ°é¢„æµ‹mask")
        return
    
    print(f"æ‰¾åˆ° {len(pred_files)} ä¸ªé¢„æµ‹mask")
    
    # è®¡ç®—æ¯å¼ å›¾ç‰‡çš„æŒ‡æ ‡
    all_metrics = []
    missing_gt = []
    
    for pred_file in tqdm(pred_files, desc="è¯„ä¼°ä¸­"):
        # æŸ¥æ‰¾å¯¹åº”çš„GTæ–‡ä»¶
        gt_file = gt_dir / pred_file.name
        
        if not gt_file.exists():
            missing_gt.append(pred_file.name)
            continue
        
        # è¯»å–maskï¼ˆé¢„æµ‹maskæ˜¯0/255ï¼ŒGT maskå¯èƒ½æ˜¯0/1æˆ–0/255ï¼‰
        pred_mask = np.array(Image.open(pred_file).convert('L')) > 127
        gt_array = np.array(Image.open(gt_file).convert('L'))
        # è‡ªåŠ¨æ£€æµ‹GTçš„å€¼èŒƒå›´æ¥ç¡®å®šé˜ˆå€¼
        gt_max = gt_array.max()
        gt_mask = gt_array > (gt_max / 2.0)
        
        # è°ƒæ•´å°ºå¯¸ï¼ˆå¦‚æœä¸åŒ¹é…ï¼‰
        if pred_mask.shape != gt_mask.shape:
            from PIL import Image as PILImage
            pred_pil = PILImage.fromarray((pred_mask * 255).astype(np.uint8))
            pred_pil = pred_pil.resize((gt_mask.shape[1], gt_mask.shape[0]), PILImage.NEAREST)
            pred_mask = np.array(pred_pil) > 127
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = calculate_metrics(pred_mask, gt_mask)
        metrics['filename'] = pred_file.name
        all_metrics.append(metrics)
    
    if missing_gt:
        print(f"\nâš ï¸  è­¦å‘Š: {len(missing_gt)} ä¸ªæ–‡ä»¶æœªæ‰¾åˆ°å¯¹åº”çš„GT")
    
    if len(all_metrics) == 0:
        print("âŒ æ²¡æœ‰å¯è¯„ä¼°çš„å›¾ç‰‡")
        return
    
    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    avg_metrics = {
        'iou': np.mean([m['iou'] for m in all_metrics]),
        'dice': np.mean([m['dice'] for m in all_metrics]),
        'accuracy': np.mean([m['accuracy'] for m in all_metrics]),
        'precision': np.mean([m['precision'] for m in all_metrics]),
        'recall': np.mean([m['recall'] for m in all_metrics]),
        'f1': np.mean([m['f1'] for m in all_metrics])
    }
    
    # è®¡ç®—æ ‡å‡†å·®
    std_metrics = {
        'iou_std': np.std([m['iou'] for m in all_metrics]),
        'dice_std': np.std([m['dice'] for m in all_metrics]),
        'accuracy_std': np.std([m['accuracy'] for m in all_metrics]),
        'precision_std': np.std([m['precision'] for m in all_metrics]),
        'recall_std': np.std([m['recall'] for m in all_metrics]),
        'f1_std': np.std([m['f1'] for m in all_metrics])
    }
    
    # æ‰“å°ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“Š è¯„ä¼°ç»“æœ")
    print("=" * 60)
    print(f"è¯„ä¼°å›¾ç‰‡æ•°: {len(all_metrics)}")
    print("-" * 60)
    print(f"IoU (Jaccard):  {avg_metrics['iou']:.4f} Â± {std_metrics['iou_std']:.4f}")
    print(f"Diceç³»æ•°:       {avg_metrics['dice']:.4f} Â± {std_metrics['dice_std']:.4f}")
    print(f"å‡†ç¡®ç‡:         {avg_metrics['accuracy']:.4f} Â± {std_metrics['accuracy_std']:.4f}")
    print(f"ç²¾ç¡®ç‡:         {avg_metrics['precision']:.4f} Â± {std_metrics['precision_std']:.4f}")
    print(f"å¬å›ç‡:         {avg_metrics['recall']:.4f} Â± {std_metrics['recall_std']:.4f}")
    print(f"F1åˆ†æ•°:         {avg_metrics['f1']:.4f} Â± {std_metrics['f1_std']:.4f}")
    print("=" * 60)
    
    # æ‰¾å‡ºæœ€å¥½å’Œæœ€å·®çš„æ ·æœ¬
    sorted_by_dice = sorted(all_metrics, key=lambda x: x['dice'], reverse=True)
    print("\nğŸ† Diceç³»æ•°æœ€é«˜çš„5ä¸ªæ ·æœ¬:")
    for i, m in enumerate(sorted_by_dice[:5], 1):
        print(f"  {i}. {m['filename']}: {m['dice']:.4f}")
    
    print("\nâš ï¸  Diceç³»æ•°æœ€ä½çš„5ä¸ªæ ·æœ¬:")
    for i, m in enumerate(sorted_by_dice[-5:], 1):
        print(f"  {i}. {m['filename']}: {m['dice']:.4f}")
    
    # ä¿å­˜ç»“æœåˆ°JSON
    if output_file:
        output_file = Path(output_file)
        results = {
            'summary': {
                'num_images': len(all_metrics),
                'average': avg_metrics,
                'std': std_metrics
            },
            'per_image': all_metrics
        }
        
        with open(output_file, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"\nâœ… è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")


def main():
    parser = argparse.ArgumentParser(description='è¯„ä¼°è¡€ç®¡åˆ†å‰²ç»“æœ')
    parser.add_argument('--pred_dir', type=str, required=True,
                        help='é¢„æµ‹maskç›®å½•')
    parser.add_argument('--gt_dir', type=str, required=True,
                        help='çœŸå®maskç›®å½•')
    parser.add_argument('--output', type=str, default='evaluation_results.json',
                        help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--flag', type=str, default=None,
                        help='å®éªŒæ ‡è¯†flag, è‹¥æä¾›åˆ™åœ¨è¾“å‡ºç›®å½•ä¸‹åˆ›å»ºè¯¥å­ç›®å½•ç”¨äºåŒºåˆ†ä¿å­˜')
    
    args = parser.parse_args()
    
    # å¤„ç†flag: è‹¥æä¾›ï¼Œå°†è¾“å‡ºæ–‡ä»¶æ”¾åˆ°æŒ‡å®šçˆ¶ç›®å½•ä¸‹çš„flagå­ç›®å½•
    resolved_output = args.output
    if args.flag:
        out_path = Path(args.output)
        parent = out_path.parent if out_path.parent.name != '' else Path('.')
        resolved_output = str(parent / args.flag / out_path.name)
        Path(parent / args.flag).mkdir(parents=True, exist_ok=True)

    evaluate(args.pred_dir, args.gt_dir, resolved_output)


if __name__ == '__main__':
    main()
